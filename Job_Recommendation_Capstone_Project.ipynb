{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Job Recommendation_Capstone Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qHyEoQ75ShV",
        "scrolled": true,
        "outputId": "86e553af-8394-4558-e504-c5ff8ceac642"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ-QomTZkIo0",
        "outputId": "b90469dc-9d06-4012-f4e1-e21aa7f6433a"
      },
      "source": [
        "!pip install -q streamlit"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.0MB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 50.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.6MB 46.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 55.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 55.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "4slGYnfmjVY3",
        "outputId": "8ccf5eea-d479-4b83-bee5-80115258b1bf"
      },
      "source": [
        "import requests\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import streamlit as st\n",
        "#use the below library while displaying the images in jupyter notebook\n",
        "from IPython.display import display, Imag"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ContextualVersionConflict",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-959cd360eb52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#use the below library while displaying the images in jupyter notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/streamlit/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# This used to be pkg_resources.require('streamlit') but it would cause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# pex files to fail. See #394 for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"streamlit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_contextlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    775\u001b[0m                 \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mContextualVersionConflict\u001b[0m: (ipykernel 4.10.1 (/usr/local/lib/python3.7/dist-packages), Requirement.parse('ipykernel>=5.1.2; python_version >= \"3.4\"'), {'pydeck'})"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Su8dyTzA9gg"
      },
      "source": [
        "import numpy as np                # mathematical calculations\n",
        "import pandas as pd               # manipulation of raw data \n",
        "import matplotlib.pyplot as plt   # plotting graphs\n",
        "%matplotlib inline "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cCWS3R49Zbx"
      },
      "source": [
        "path = '/gdrive/My Drive/Aegis_Capstone_Project/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7h-cntP9pMj"
      },
      "source": [
        "final_jobs = pd.read_csv(path+'Combined_Jobs_Final.csv')\n",
        "final_jobs.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2Eru0_AjUHK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMnpvi6OKtao"
      },
      "source": [
        "final_jobs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syAONhfV5Shi"
      },
      "source": [
        "# Listing out all the columns that are present in the data set.\n",
        "\n",
        "list(final_jobs) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4NhDaBT5Shm"
      },
      "source": [
        "print(final_jobs.shape)\n",
        "final_jobs.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWsPxpOd5Shq"
      },
      "source": [
        " From the above list we see that there are lot of NaN values, perform data cleansing for each and every column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHv0xRNm5Shs"
      },
      "source": [
        "## Concatenating the columns ( job corups)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VACncfdT5Shu"
      },
      "source": [
        "#subsetting only needed columns and not considering the columns that are not necessary\n",
        "cols = list(['Job.ID']+['Slug']+['Title']+['Position']+ ['Company']+['City']+['Employment.Type']+['Education.Required']+['Job.Description'])\n",
        "final_jobs =final_jobs[cols]\n",
        "final_jobs.columns = ['Job.ID','Slug', 'Title', 'Position', 'Company','City', 'Empl_type','Edu_req','Job_Description']\n",
        "final_jobs.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rau3EyTb5Shw"
      },
      "source": [
        "# checking for the null values again.\n",
        "final_jobs.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1DfquB9K2z3"
      },
      "source": [
        "final_jobs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aETRWUJH5Shy"
      },
      "source": [
        "#selecting NaN rows of city\n",
        "nan_city = final_jobs[pd.isnull(final_jobs['City'])]\n",
        "print(nan_city.shape)\n",
        "nan_city.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faCoxuKw5Sh0",
        "scrolled": false
      },
      "source": [
        "nan_city.groupby(['Company'])['City'].count() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxvil75c5Sh2"
      },
      "source": [
        "We see that there are only 9 companies cities that are having NaN values so I manually adding their head quarters with the help of google search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA_BgVN25Sh4",
        "scrolled": true
      },
      "source": [
        "#replacing nan with thier headquarters location\n",
        "final_jobs['Company'] = final_jobs['Company'].replace(['Genesis Health Systems'], 'Genesis Health System')\n",
        "\n",
        "final_jobs.loc[final_jobs.Company == 'CHI Payment Systems', 'City'] = 'Illinois'\n",
        "final_jobs.loc[final_jobs.Company == 'Academic Year In America', 'City'] = 'Stamford'\n",
        "final_jobs.loc[final_jobs.Company == 'CBS Healthcare Services and Staffing ', 'City'] = 'Urbandale'\n",
        "final_jobs.loc[final_jobs.Company == 'Driveline Retail', 'City'] = 'Coppell'\n",
        "final_jobs.loc[final_jobs.Company == 'Educational Testing Services', 'City'] = 'New Jersey'\n",
        "final_jobs.loc[final_jobs.Company == 'Genesis Health System', 'City'] = 'Davennport'\n",
        "final_jobs.loc[final_jobs.Company == 'Home Instead Senior Care', 'City'] = 'Nebraska'\n",
        "final_jobs.loc[final_jobs.Company == 'St. Francis Hospital', 'City'] = 'New York'\n",
        "final_jobs.loc[final_jobs.Company == 'Volvo Group', 'City'] = 'Washington'\n",
        "final_jobs.loc[final_jobs.Company == 'CBS Healthcare Services and Staffing', 'City'] = 'Urbandale'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY6SVC0L5Sh9"
      },
      "source": [
        "final_jobs.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjHJpRSw5SiA",
        "scrolled": false
      },
      "source": [
        "#The employement type NA are from Uber so I assume as part-time and full time\n",
        "nan_emp_type = final_jobs[pd.isnull(final_jobs['Empl_type'])]\n",
        "print(nan_emp_type)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upcxmN4E5SiC"
      },
      "source": [
        "#replacing na values with part time/full time\n",
        "final_jobs['Empl_type']=final_jobs['Empl_type'].fillna('Full-Time/Part-Time')\n",
        "final_jobs.groupby(['Empl_type'])['Company'].count()\n",
        "list(final_jobs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXOTJdR95SiE"
      },
      "source": [
        "#   corpus "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqUUFJPO5SiG"
      },
      "source": [
        "#### combining the columns of position,company,city,emp_type and position"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSMkmgYf5SiH",
        "scrolled": true
      },
      "source": [
        "final_jobs[\"pos_com_city_empType_jobDesc\"] = final_jobs[\"Position\"].map(str) + \" \" + final_jobs[\"Company\"] +\" \"+ final_jobs[\"City\"]+ \" \"+final_jobs['Empl_type']+\" \"+final_jobs['Job_Description']\n",
        "final_jobs.pos_com_city_empType_jobDesc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6EEJIMH5SiJ"
      },
      "source": [
        "#removing unnecessary characters between words separated by space between each word of all columns to make the data efficient\n",
        "final_jobs['pos_com_city_empType_jobDesc'] = final_jobs['pos_com_city_empType_jobDesc'].str.replace('[^a-zA-Z \\n\\.]',\" \") #removing unnecessary characters\n",
        "final_jobs.pos_com_city_empType_jobDesc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-WoAvd-5SiL"
      },
      "source": [
        "#converting all the characeters to lower case\n",
        "final_jobs['pos_com_city_empType_jobDesc'] = final_jobs['pos_com_city_empType_jobDesc'].str.lower() \n",
        "final_jobs.pos_com_city_empType_jobDesc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmWwSWBX5SiM"
      },
      "source": [
        "final_all = final_jobs[['Job.ID', 'pos_com_city_empType_jobDesc']]\n",
        "# renaming the column name as it seemed a bit complicated\n",
        "final_all = final_jobs[['Job.ID', 'pos_com_city_empType_jobDesc']]\n",
        "final_all = final_all.fillna(\" \")\n",
        "\n",
        "final_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9xdxhBg5Si0"
      },
      "source": [
        "The next package used in here is stemming, The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved. So in order to clean up the space, we use stemming method and the one of the packages used here is PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0VDISxk5Si2"
      },
      "source": [
        "print(final_all.head(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6jkiMIZ5Si3"
      },
      "source": [
        "  ##  NLTK Process\n",
        "It removes stop words such as the, is, and etc.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm91niahRvII"
      },
      "source": [
        "# Setup\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I92bypcm5Si4"
      },
      "source": [
        "pos_com_city_empType_jobDesc = final_all['pos_com_city_empType_jobDesc']\n",
        "#removing stopwords and applying potter stemming\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer =  PorterStemmer()\n",
        "stop = stopwords.words('english')\n",
        "only_text = pos_com_city_empType_jobDesc.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "only_text.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvKM7ijN5Si6"
      },
      "source": [
        "Splitting each word in a row separated by space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGn8sBe55Si7"
      },
      "source": [
        "only_text = only_text.apply(lambda x : filter(None,x.split(\" \")))\n",
        "print(only_text.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if6xYRXh5Si-"
      },
      "source": [
        "Here **stemming** is basically used to remove the suffixes and common words that repeat and separated by commas. for y in x means, for each word(y) in the total list(x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuNRIJxQ5SjA"
      },
      "source": [
        "only_text = only_text.apply(lambda x : [stemmer.stem(y) for y in x])\n",
        "print(only_text.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SwuMiTm5SjC"
      },
      "source": [
        "In the above code, we separated each letter in a word separated by comma, now, in this step, we join the words(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEDWi2Wu5SjD"
      },
      "source": [
        "only_text = only_text.apply(lambda x : \" \".join(x))\n",
        "print(only_text.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bonJfJ-u5SjE"
      },
      "source": [
        "#adding the featured column back to pandas\n",
        "final_all['text']= only_text\n",
        "# As we have added a new column by performing all the operations using lambda function, we are removing the unnecessary column\n",
        "#final_all = final_all.drop(\"pos_com_city_empType_jobDesc\", 1)\n",
        "\n",
        "list(final_all)\n",
        "final_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2u4SPQ05SjG"
      },
      "source": [
        "# in order to save this file for a backup\n",
        "#final_all.to_csv(\"job_data.csv\", index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLJr-Dso5SjH"
      },
      "source": [
        "# TF-IDF ( Term Frequency - Inverse Document Frequency ) \n",
        "This method is also called as Normalization.\n",
        "TF - How many times a particular word appears in a single doc. \n",
        "IDF - This downscales words that appear a lot across documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "defKc6Il5SjK"
      },
      "source": [
        "There is a difference between fit (unique words are created / vectorization) and fit_transform.\n",
        "fit means fit transform and transform( adding the row internally adding 1 in the users input query)\n",
        "ex : innovate is not present then then it will not create ( loosing the efficiency)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8THNvrr95SjL"
      },
      "source": [
        "\n",
        "#initializing tfidf vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_jobid = tfidf_vectorizer.fit_transform((final_all['text'])) #fitting and transforming the vector\n",
        "tfidf_jobid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZIcFt695SjN"
      },
      "source": [
        "# User query Corpus\n",
        "Take another dataset called job views."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CURMnPpR5SjO"
      },
      "source": [
        "#Consider a  new data set and  taking the datasets job view, position of interest, experience of the applicant into consideration for creating a query who applied for job\n",
        "job_view = pd.read_csv(path+'Job_Views.csv')\n",
        "job_view.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3vzg9Cq5SjQ",
        "scrolled": true
      },
      "source": [
        "#subsetting only needed columns and not considering the columns that are not necessary as we did that earlier.\n",
        "job_view = job_view[['Applicant.ID', 'Job.ID', 'Position', 'Company','City']]\n",
        "\n",
        "job_view[\"pos_com_city\"] = job_view[\"Position\"].map(str) + \"  \" + job_view[\"Company\"] +\"  \"+ job_view[\"City\"]\n",
        "\n",
        "job_view['pos_com_city'] = job_view['pos_com_city'].str.replace('[^a-zA-Z \\n\\.]',\"\")\n",
        "\n",
        "job_view['pos_com_city'] = job_view['pos_com_city'].str.lower()\n",
        "\n",
        "job_view = job_view[['Applicant.ID','pos_com_city']]\n",
        "\n",
        "job_view.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E20Q0WII5SjS"
      },
      "source": [
        "### Experience\n",
        "We take experience of all the applicants who applied for the job and we are comaring the point of interest with the jobs that are present in our previous data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJx72mCc5SjT"
      },
      "source": [
        "#Experience\n",
        "exper_applicant = pd.read_csv(path+'Experience.csv')\n",
        "exper_applicant.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaKALNyh5SjU"
      },
      "source": [
        "#taking only Position\n",
        "exper_applicant = exper_applicant[['Applicant.ID','Position.Name']]\n",
        "\n",
        "#cleaning the text\n",
        "exper_applicant['Position.Name'] = exper_applicant['Position.Name'].str.replace('[^a-zA-Z \\n\\.]',\"\")\n",
        "\n",
        "exper_applicant.head()\n",
        "#list(exper_applicant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO0lvd8S5SjW"
      },
      "source": [
        "exper_applicant['Position.Name'] = exper_applicant['Position.Name'].str.lower()\n",
        "exper_applicant.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOMtLq2j5SjY"
      },
      "source": [
        "exper_applicant =  exper_applicant.sort_values(by='Applicant.ID')\n",
        "exper_applicant = exper_applicant.fillna(\" \")\n",
        "exper_applicant.head(20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ofdPD_5SjZ"
      },
      "source": [
        "same applicant has 3 applications 100001 in sigle line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_hAx8B75Sja"
      },
      "source": [
        "#adding same rows to a single row\n",
        "exper_applicant = exper_applicant.groupby('Applicant.ID', sort=False)['Position.Name'].apply(' '.join).reset_index()\n",
        "exper_applicant.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN6XC4f_5Sjc"
      },
      "source": [
        "### Position of Interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEmMxZXH5Sjd",
        "scrolled": true
      },
      "source": [
        "#Position of interest\n",
        "poi =  pd.read_csv(path+'Positions_Of_Interest.csv')\n",
        "poi = poi.sort_values(by='Applicant.ID')\n",
        "poi.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN1E95vX5Sje"
      },
      "source": [
        "# There is no need of application and updation becuase there is no deadline mentioned in the website ( assumption) hence we are droping unimportant attributes\n",
        "poi = poi.drop('Updated.At', 1)\n",
        "poi = poi.drop('Created.At', 1)\n",
        "\n",
        "#cleaning the text\n",
        "poi['Position.Of.Interest']=poi['Position.Of.Interest'].str.replace('[^a-zA-z \\n\\.]',\"\")\n",
        "poi['Position.Of.Interest']=poi['Position.Of.Interest'].str.lower()\n",
        "poi = poi.fillna(\" \")\n",
        "poi.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQW8LDTg5Sjg"
      },
      "source": [
        "poi = poi.groupby('Applicant.ID', sort=True)['Position.Of.Interest'].apply(' '.join).reset_index()\n",
        "poi.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EjgLlZQ5Sjm"
      },
      "source": [
        "### Merging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1_treob5Sjn"
      },
      "source": [
        "pos_com city, position_name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqgcq1UU5Sjp",
        "scrolled": true
      },
      "source": [
        "#merging jobs and experience dataframes\n",
        "out_joint_jobs = job_view.merge(exper_applicant, left_on='Applicant.ID', right_on='Applicant.ID', how='outer')\n",
        "print(out_joint_jobs.shape)\n",
        "out_joint_jobs = out_joint_jobs.fillna(' ')\n",
        "out_joint_jobs = out_joint_jobs.sort_values(by='Applicant.ID')\n",
        "out_joint_jobs.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lfgemse5Sjs"
      },
      "source": [
        "#merging position of interest with existing dataframe\n",
        "joint_poi_exper_view = out_joint_jobs.merge(poi, left_on='Applicant.ID', right_on='Applicant.ID', how='outer')\n",
        "joint_poi_exper_view = joint_poi_exper_view.fillna(' ')\n",
        "joint_poi_exper_view = joint_poi_exper_view.sort_values(by='Applicant.ID')\n",
        "joint_poi_exper_view.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc1pj0dP5Sjz"
      },
      "source": [
        "#combining all the columns\n",
        "\n",
        "joint_poi_exper_view[\"pos_com_city1\"] = joint_poi_exper_view[\"pos_com_city\"].map(str) + joint_poi_exper_view[\"Position.Name\"] +\" \"+ joint_poi_exper_view[\"Position.Of.Interest\"]\n",
        "\n",
        "joint_poi_exper_view.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-CwkV9q5Sj2"
      },
      "source": [
        "final_poi_exper_view = joint_poi_exper_view[['Applicant.ID','pos_com_city1']]\n",
        "final_poi_exper_view.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lxWCWKI5Sj3"
      },
      "source": [
        "final_poi_exper_view.columns = ['Applicant_id','pos_com_city1']\n",
        "final_poi_exper_view.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQxhnGOw5Sj5"
      },
      "source": [
        "final_poi_exper_view = final_poi_exper_view.sort_values(by='Applicant_id')\n",
        "final_poi_exper_view.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYCvHdWu5Sj6"
      },
      "source": [
        "final_poi_exper_view['pos_com_city1'] = final_poi_exper_view['pos_com_city1'].str.replace('[^a-zA-Z \\n\\.]',\"\")\n",
        "final_poi_exper_view.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ym3bfXG5Sj8"
      },
      "source": [
        "final_poi_exper_view['pos_com_city1'] = final_poi_exper_view['pos_com_city1'].str.lower()\n",
        "final_poi_exper_view.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWv3mIGM5Sj-"
      },
      "source": [
        "\n",
        "final_poi_exper_view = final_poi_exper_view.reset_index(drop=True)\n",
        "final_poi_exper_view.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk2p0vwE5SkA"
      },
      "source": [
        "select random row of 6999"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syylhuuZ5SkB"
      },
      "source": [
        "#taking a user\n",
        "u = 6999\n",
        "index = np.where(final_poi_exper_view['Applicant_id'] == u)[0][0]\n",
        "user_q = final_poi_exper_view.iloc[[index]]\n",
        "user_q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmUAMxJk5SkD"
      },
      "source": [
        "# Using vector space model ( Cosine similarity )\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ww_K7ZF5SkF"
      },
      "source": [
        "#creating tf-idf of user query and computing cosine similarity of user with job corpus\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "user_tfidf = tfidf_vectorizer.transform(user_q['pos_com_city1'])\n",
        "output = map(lambda x: cosine_similarity(user_tfidf, x),tfidf_jobid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UnbeCx15SkG"
      },
      "source": [
        "output2 = list(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAfQoIEQ8-Zp"
      },
      "source": [
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIKP_RCl5SkH"
      },
      "source": [
        "#getting the job id's of the recommendations\n",
        "top = sorted(range(len(output2)), key=lambda i: output2[i], reverse=True)[:50]\n",
        "recommendation = pd.DataFrame(columns = ['ApplicantID', 'JobID'])\n",
        "count = 0\n",
        "for i in top:\n",
        "  recommendation.at[count, 'ApplicantID']=u\n",
        "  recommendation.at[count,'JobID'] = final_all['Job.ID'][i]\n",
        "  count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SSWhBiAz5YeD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqvnigGsScjD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANERDEsQ5SkI"
      },
      "source": [
        "cosine similarity score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEoG6VNW5SkK",
        "scrolled": true
      },
      "source": [
        "recommendation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iN_9GHI5SkM",
        "scrolled": true
      },
      "source": [
        "#getting the job ids and their data\n",
        "nearestjobs = recommendation['JobID']\n",
        "job_description = pd.DataFrame(columns = ['JobID','text'])\n",
        "for i in nearestjobs:\n",
        "    index = np.where(final_all['Job.ID'] == i)[0][0]    \n",
        "    job_description.at[count, 'JobID']=i\n",
        "    job_description.at[count, 'text']= final_all['text'][index]\n",
        "    count += 1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh9ez-M55SkO",
        "scrolled": true
      },
      "source": [
        "#printing the jobs that matched the query\n",
        "job_description"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwYcAwPC5SkP",
        "scrolled": true
      },
      "source": [
        "job_description.to_csv(\"recommended_content.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTYOZftk5SkQ"
      },
      "source": [
        "final_all.to_csv(\"job_data.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhJFmz_l_qZF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}